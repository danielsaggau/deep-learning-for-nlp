---
title: "Goodfellow_Summary"
author: "Daniel Saggau"
date: "2/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 5 Machine Learning Basics



# Chapter 7 Regularization for Deep Learning 

## 7.1 Parameter Norm Penalties

### 7.1.1 $L^2$ Parameter Regularization 

### 7.1.2 $L^1$ Regularization 


## 7.2 Norm Penalties as Constrained Optimization 


## 7.3 Regularization and Under-Constrained Problems 


## 7.7 Multi-Task Learning 


## 7.8 Early Stopping 

## 7.9 Parameter Tying and Parameter Sharing


## 7.10 Sparse Representations

## 7.11 Bagging and Other Ensemble Methods

## 7.12 Dropout

## 

# Chapter 8 



# Chapter 9 



# Chapter 10 Sequence Modeling: Recurrent and Recursive Nets 

## 10.1 Unfolding Computational Graphs 

## 10.2 Recurrent Neural Networks 

### 10.2.1 Teacher Forcing and Networks with Output Recurrence 



### 10.2.2 Computing the Gradient in a Recurrent Neural Network



### 10.2.3 Recurrent Networks as Directed Graphical Models 



### 10.2.4 Modeling Sequences Conditioned on Context with RNNs



## 10.3 Bidirectional RNNs



## 10.4 Encoder-Decoder Sequence-to-Sequence Architectures



## 10.5 Deep Recurrent Networks



## 10.6 Recursive Neural Networks



## 10.7 The Challenge of Long-Term Dependencies



## 10.8 Echo State Networks



## 10.9 Leaky Units and Other Strategies for Multiple Time Scales



### 10.9.1 Adding Skip Connections through Time



### 10.9.2 Leaky Units and a Spectrum of Different Time Scales



### 10.9.3 Removing Connections



## 10.10 The Long Short-Term Memory and Other Gated RNNs



### 10.10.1 LSTM



## 10.10.2 Other Gated RNNs



## 10.11 Optimization for Long-Term Dependencies



### 10.11.1 Clipping Gradients



### 10.11.2 Regularizing to Encourage Information Flow



## 10.12 Explicit Memory
